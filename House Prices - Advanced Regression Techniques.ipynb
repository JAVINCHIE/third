{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a5ae1ee-4574-4108-8803-8f52fb5d7943",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape   \n",
      "0  1461          20       RH         80.0    11622   Pave   NaN      Reg  \\\n",
      "1  1462          20       RL         81.0    14267   Pave   NaN      IR1   \n",
      "2  1463          60       RL         74.0    13830   Pave   NaN      IR1   \n",
      "3  1464          60       RL         78.0     9978   Pave   NaN      IR1   \n",
      "4  1465         120       RL         43.0     5005   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... ScreenPorch PoolArea PoolQC  Fence MiscFeature   \n",
      "0         Lvl    AllPub  ...         120        0    NaN  MnPrv         NaN  \\\n",
      "1         Lvl    AllPub  ...           0        0    NaN    NaN        Gar2   \n",
      "2         Lvl    AllPub  ...           0        0    NaN  MnPrv         NaN   \n",
      "3         Lvl    AllPub  ...           0        0    NaN    NaN         NaN   \n",
      "4         HLS    AllPub  ...         144        0    NaN    NaN         NaN   \n",
      "\n",
      "  MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
      "0       0      6    2010        WD         Normal  \n",
      "1   12500      6    2010        WD         Normal  \n",
      "2       0      3    2010        WD         Normal  \n",
      "3       0      6    2010        WD         Normal  \n",
      "4       0      1    2010        WD         Normal  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape   \n",
      "0   1          60       RL         65.0     8450   Pave   NaN      Reg  \\\n",
      "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
      "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
      "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
      "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold   \n",
      "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2  \\\n",
      "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
      "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
      "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
      "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
      "\n",
      "  YrSold  SaleType  SaleCondition  SalePrice  \n",
      "0   2008        WD         Normal     208500  \n",
      "1   2007        WD         Normal     181500  \n",
      "2   2008        WD         Normal     223500  \n",
      "3   2006        WD        Abnorml     140000  \n",
      "4   2008        WD         Normal     250000  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Display the first few rows of each dataset to understand their structure\n",
    "print(test_df.head())\n",
    "print(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c454604f-1312-4388-8813-106a176b7702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 80 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1459 non-null   int64  \n",
      " 1   MSSubClass     1459 non-null   int64  \n",
      " 2   MSZoning       1455 non-null   object \n",
      " 3   LotFrontage    1232 non-null   float64\n",
      " 4   LotArea        1459 non-null   int64  \n",
      " 5   Street         1459 non-null   object \n",
      " 6   Alley          107 non-null    object \n",
      " 7   LotShape       1459 non-null   object \n",
      " 8   LandContour    1459 non-null   object \n",
      " 9   Utilities      1457 non-null   object \n",
      " 10  LotConfig      1459 non-null   object \n",
      " 11  LandSlope      1459 non-null   object \n",
      " 12  Neighborhood   1459 non-null   object \n",
      " 13  Condition1     1459 non-null   object \n",
      " 14  Condition2     1459 non-null   object \n",
      " 15  BldgType       1459 non-null   object \n",
      " 16  HouseStyle     1459 non-null   object \n",
      " 17  OverallQual    1459 non-null   int64  \n",
      " 18  OverallCond    1459 non-null   int64  \n",
      " 19  YearBuilt      1459 non-null   int64  \n",
      " 20  YearRemodAdd   1459 non-null   int64  \n",
      " 21  RoofStyle      1459 non-null   object \n",
      " 22  RoofMatl       1459 non-null   object \n",
      " 23  Exterior1st    1458 non-null   object \n",
      " 24  Exterior2nd    1458 non-null   object \n",
      " 25  MasVnrType     565 non-null    object \n",
      " 26  MasVnrArea     1444 non-null   float64\n",
      " 27  ExterQual      1459 non-null   object \n",
      " 28  ExterCond      1459 non-null   object \n",
      " 29  Foundation     1459 non-null   object \n",
      " 30  BsmtQual       1415 non-null   object \n",
      " 31  BsmtCond       1414 non-null   object \n",
      " 32  BsmtExposure   1415 non-null   object \n",
      " 33  BsmtFinType1   1417 non-null   object \n",
      " 34  BsmtFinSF1     1458 non-null   float64\n",
      " 35  BsmtFinType2   1417 non-null   object \n",
      " 36  BsmtFinSF2     1458 non-null   float64\n",
      " 37  BsmtUnfSF      1458 non-null   float64\n",
      " 38  TotalBsmtSF    1458 non-null   float64\n",
      " 39  Heating        1459 non-null   object \n",
      " 40  HeatingQC      1459 non-null   object \n",
      " 41  CentralAir     1459 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1459 non-null   int64  \n",
      " 44  2ndFlrSF       1459 non-null   int64  \n",
      " 45  LowQualFinSF   1459 non-null   int64  \n",
      " 46  GrLivArea      1459 non-null   int64  \n",
      " 47  BsmtFullBath   1457 non-null   float64\n",
      " 48  BsmtHalfBath   1457 non-null   float64\n",
      " 49  FullBath       1459 non-null   int64  \n",
      " 50  HalfBath       1459 non-null   int64  \n",
      " 51  BedroomAbvGr   1459 non-null   int64  \n",
      " 52  KitchenAbvGr   1459 non-null   int64  \n",
      " 53  KitchenQual    1458 non-null   object \n",
      " 54  TotRmsAbvGrd   1459 non-null   int64  \n",
      " 55  Functional     1457 non-null   object \n",
      " 56  Fireplaces     1459 non-null   int64  \n",
      " 57  FireplaceQu    729 non-null    object \n",
      " 58  GarageType     1383 non-null   object \n",
      " 59  GarageYrBlt    1381 non-null   float64\n",
      " 60  GarageFinish   1381 non-null   object \n",
      " 61  GarageCars     1458 non-null   float64\n",
      " 62  GarageArea     1458 non-null   float64\n",
      " 63  GarageQual     1381 non-null   object \n",
      " 64  GarageCond     1381 non-null   object \n",
      " 65  PavedDrive     1459 non-null   object \n",
      " 66  WoodDeckSF     1459 non-null   int64  \n",
      " 67  OpenPorchSF    1459 non-null   int64  \n",
      " 68  EnclosedPorch  1459 non-null   int64  \n",
      " 69  3SsnPorch      1459 non-null   int64  \n",
      " 70  ScreenPorch    1459 non-null   int64  \n",
      " 71  PoolArea       1459 non-null   int64  \n",
      " 72  PoolQC         3 non-null      object \n",
      " 73  Fence          290 non-null    object \n",
      " 74  MiscFeature    51 non-null     object \n",
      " 75  MiscVal        1459 non-null   int64  \n",
      " 76  MoSold         1459 non-null   int64  \n",
      " 77  YrSold         1459 non-null   int64  \n",
      " 78  SaleType       1458 non-null   object \n",
      " 79  SaleCondition  1459 non-null   object \n",
      "dtypes: float64(11), int64(26), object(43)\n",
      "memory usage: 912.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     588 non-null    object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display basic information about each dataset to understand data types and missing values\n",
    "print(test_df.info())\n",
    "print(train_df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d62631f2-3f3a-42ed-8af1-350b7fe07259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in test dataset:\n",
      "              Missing Values  % of Total Values\n",
      "PoolQC                  1456               99.8\n",
      "MiscFeature             1408               96.5\n",
      "Alley                   1352               92.7\n",
      "Fence                   1169               80.1\n",
      "MasVnrType               894               61.3\n",
      "FireplaceQu              730               50.0\n",
      "LotFrontage              227               15.6\n",
      "GarageCond                78                5.3\n",
      "GarageYrBlt               78                5.3\n",
      "GarageQual                78                5.3\n",
      "GarageFinish              78                5.3\n",
      "GarageType                76                5.2\n",
      "BsmtCond                  45                3.1\n",
      "BsmtExposure              44                3.0\n",
      "BsmtQual                  44                3.0\n",
      "BsmtFinType1              42                2.9\n",
      "BsmtFinType2              42                2.9\n",
      "MasVnrArea                15                1.0\n",
      "MSZoning                   4                0.3\n",
      "BsmtFullBath               2                0.1\n",
      "BsmtHalfBath               2                0.1\n",
      "Functional                 2                0.1\n",
      "Utilities                  2                0.1\n",
      "GarageCars                 1                0.1\n",
      "GarageArea                 1                0.1\n",
      "TotalBsmtSF                1                0.1\n",
      "KitchenQual                1                0.1\n",
      "BsmtUnfSF                  1                0.1\n",
      "BsmtFinSF2                 1                0.1\n",
      "BsmtFinSF1                 1                0.1\n",
      "Exterior2nd                1                0.1\n",
      "Exterior1st                1                0.1\n",
      "SaleType                   1                0.1\n",
      "\n",
      "Missing values in train dataset:\n",
      "              Missing Values  % of Total Values\n",
      "PoolQC                  1453               99.5\n",
      "MiscFeature             1406               96.3\n",
      "Alley                   1369               93.8\n",
      "Fence                   1179               80.8\n",
      "MasVnrType               872               59.7\n",
      "FireplaceQu              690               47.3\n",
      "LotFrontage              259               17.7\n",
      "GarageType                81                5.5\n",
      "GarageYrBlt               81                5.5\n",
      "GarageFinish              81                5.5\n",
      "GarageQual                81                5.5\n",
      "GarageCond                81                5.5\n",
      "BsmtFinType2              38                2.6\n",
      "BsmtExposure              38                2.6\n",
      "BsmtFinType1              37                2.5\n",
      "BsmtCond                  37                2.5\n",
      "BsmtQual                  37                2.5\n",
      "MasVnrArea                 8                0.5\n",
      "Electrical                 1                0.1\n"
     ]
    }
   ],
   "source": [
    "# Function to display missing values\n",
    "def missing_values_table(df):\n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns={0: 'Missing Values', 1: '% of Total Values'})\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "    print(mis_val_table_ren_columns)\n",
    "\n",
    "# Display missing values in each dataset\n",
    "print(\"Missing values in test dataset:\")\n",
    "missing_values_table(test_df)\n",
    "print(\"\\nMissing values in train dataset:\")\n",
    "missing_values_table(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a83d0f27-ea38-468f-bb5d-d99c25ad2316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling missing values:\n",
      "Id               0\n",
      "MSSubClass       0\n",
      "MSZoning         0\n",
      "LotFrontage      0\n",
      "LotArea          0\n",
      "                ..\n",
      "MiscVal          0\n",
      "MoSold           0\n",
      "YrSold           0\n",
      "SaleType         1\n",
      "SaleCondition    0\n",
      "Length: 76, dtype: int64\n",
      "Id               0\n",
      "MSSubClass       0\n",
      "MSZoning         0\n",
      "LotFrontage      0\n",
      "LotArea          0\n",
      "                ..\n",
      "MoSold           0\n",
      "YrSold           0\n",
      "SaleType         0\n",
      "SaleCondition    0\n",
      "SalePrice        0\n",
      "Length: 77, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values\n",
    "# Fill missing values for numerical columns with median\n",
    "test_df['LotFrontage'] = test_df['LotFrontage'].fillna(test_df['LotFrontage'].median())\n",
    "train_df['LotFrontage'] = train_df['LotFrontage'].fillna(train_df['LotFrontage'].median())\n",
    "\n",
    "# Fill missing values for categorical columns with mode\n",
    "test_df['MSZoning'] = test_df['MSZoning'].fillna(test_df['MSZoning'].mode()[0])\n",
    "train_df['MSZoning'] = train_df['MSZoning'].fillna(train_df['MSZoning'].mode()[0])\n",
    "\n",
    "# Drop columns with too many missing values\n",
    "test_df = test_df.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "train_df = train_df.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n",
    "\n",
    "# Verify if missing values have been handled\n",
    "print(\"After handling missing values:\")\n",
    "print(test_df.isnull().sum())\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7e8f63f-ccef-44ef-82ef-3a8562e8005a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_csv(\"test_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9691c97-61fe-41ba-a3b7-4ee67f67d4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0916ddc-1c4f-46ba-bef6-7ca8fbf72ba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 900 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n180 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n    super().fit(\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 186, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 579, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'RM'\n\n--------------------------------------------------------------------------------\n720 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n    super().fit(\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 186, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 579, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'RL'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Perform grid search with cross-validation\u001b[39;00m\n\u001b[0;32m     29\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mdt, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Get the best model\u001b[39;00m\n\u001b[0;32m     33\u001b[0m best_dt \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 900 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n180 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n    super().fit(\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 186, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 579, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'RM'\n\n--------------------------------------------------------------------------------\n720 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1247, in fit\n    super().fit(\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 186, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 579, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_X_params)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1998, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'RL'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Load the updated datasets\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Initialize the DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_dt = grid_search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = best_dt.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_rmse = val_mse ** 0.5\n",
    "\n",
    "# Predict on test set\n",
    "X_test = test_df.drop(['Id'], axis=1)\n",
    "test_df['SalePrice'] = best_dt.predict(X_test)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Test Predictions\", dataframe=test_df)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('/mnt/data/test_predictions.csv', index=False)\n",
    "\n",
    "val_rmse, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc2f8e-e9d4-4616-9bd7-11a36651f0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the test predictions\n",
    "#import ace_tools as tools\n",
    "\n",
    "#tools.display_dataframe_to_user(name=\"Test Predictions\", dataframe=test_df[['Id', 'SalePrice']])\n",
    "\n",
    "# Return the best parameters and RMSE\n",
    "val_rmse, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5d9aa-9c3b-4509-a981-139487ac7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['Id', 'SalePrice']].to_csv('/mnt/data/test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08eb4a8c-78cf-4a47-8697-6700a82b7fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'model__max_depth': 7,\n",
       "  'model__max_features': 'auto',\n",
       "  'model__min_samples_leaf': 2,\n",
       "  'model__min_samples_split': 10},\n",
       " -1428981414.0830266)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the datasets\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id'], axis=1)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Preprocessing pipelines for numerical and categorical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create and evaluate the pipeline with grid search\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('model', model)])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "test_df['SalePrice'] = best_model.predict(X_test)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Test Predictions\", dataframe=test_df)\n",
    "\n",
    "grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7bc1a8c1-545c-4423-9aa2-10580ee6c598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1428981414.0830266"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "906d474d-6521-4424-bb23-476a62f77d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 41170.644163996796\n",
      "Best Parameters: {'model__max_depth': None, 'model__max_features': 'auto', 'model__min_samples_leaf': 4, 'model__min_samples_split': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41170.644163996796,\n",
       " {'model__max_depth': None,\n",
       "  'model__max_features': 'auto',\n",
       "  'model__min_samples_leaf': 4,\n",
       "  'model__min_samples_split': 10})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id'], axis=1)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Preprocessing pipelines for numerical and categorical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create and evaluate the pipeline with grid search\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('model', model)])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "\n",
    "# Predict on test set\n",
    "test_df['SalePrice'] = best_model.predict(X_test)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Test Predictions\", dataframe=test_df)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Return the best parameters and RMSE\n",
    "val_rmse, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dffa0439-17cb-43b4-b980-b8667362c388",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- SalePrice\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m val_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(val_mse)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Predict on test set\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Save predictions to a new CSV file\u001b[39;00m\n\u001b[0;32m     78\u001b[0m test_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/data/test_predictions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:480\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    478\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 480\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:551\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \n\u001b[0;32m    538\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    `X` with imputed values.\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    549\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 551\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:344\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\impute\\_base.py:327\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    324\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- SalePrice\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "# Create new features\n",
    "train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "\n",
    "train_df['OverallQual_GrLivArea'] = train_df['OverallQual'] * train_df['GrLivArea']\n",
    "test_df['OverallQual_GrLivArea'] = test_df['OverallQual'] * test_df['GrLivArea']\n",
    "\n",
    "train_df['GrLivArea'] = np.log1p(train_df['GrLivArea'])\n",
    "test_df['GrLivArea'] = np.log1p(test_df['GrLivArea'])\n",
    "\n",
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "\n",
    "train_df['TotalPorchSF'] = (train_df['OpenPorchSF'] + train_df['3SsnPorch'] +\n",
    "                            train_df['EnclosedPorch'] + train_df['ScreenPorch'] + train_df['WoodDeckSF'])\n",
    "test_df['TotalPorchSF'] = (test_df['OpenPorchSF'] + test_df['3SsnPorch'] +\n",
    "                           test_df['EnclosedPorch'] + test_df['ScreenPorch'] + test_df['WoodDeckSF'])\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id'], axis=1)\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create and evaluate the pipeline with grid search\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "\n",
    "# Predict on test set\n",
    "test_df['SalePrice'] = best_model.predict(X_test)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('/mnt/data/test_predictions.csv', index=False)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Test Predictions\", dataframe=test_df)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Return the best parameters and RMSE\n",
    "val_rmse, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf0259d9-5464-44f7-83a7-62eee414a435",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Validation RMSE: 38440.0099609417\n",
      "Best Parameters: {'model__max_depth': 7, 'model__max_features': None, 'model__min_samples_leaf': 2, 'model__min_samples_split': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38440.0099609417,\n",
       " {'model__max_depth': 7,\n",
       "  'model__max_features': None,\n",
       "  'model__min_samples_leaf': 2,\n",
       "  'model__min_samples_split': 5})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "# Create new features\n",
    "train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "\n",
    "train_df['OverallQual_GrLivArea'] = train_df['OverallQual'] * train_df['GrLivArea']\n",
    "test_df['OverallQual_GrLivArea'] = test_df['OverallQual'] * test_df['GrLivArea']\n",
    "\n",
    "train_df['GrLivArea'] = np.log1p(train_df['GrLivArea'])\n",
    "test_df['GrLivArea'] = np.log1p(test_df['GrLivArea'])\n",
    "\n",
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "\n",
    "train_df['TotalPorchSF'] = (train_df['OpenPorchSF'] + train_df['3SsnPorch'] +\n",
    "                            train_df['EnclosedPorch'] + train_df['ScreenPorch'] + train_df['WoodDeckSF'])\n",
    "test_df['TotalPorchSF'] = (test_df['OpenPorchSF'] + test_df['3SsnPorch'] +\n",
    "                           test_df['EnclosedPorch'] + test_df['ScreenPorch'] + test_df['WoodDeckSF'])\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id', 'SalePrice'], axis=1, errors='ignore')\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create and evaluate the pipeline with grid search\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2', None]  # 'auto' removed to avoid warning\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "\n",
    "# Predict on test set\n",
    "test_df['SalePrice'] = best_model.predict(X_test)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('test_predictions100.csv', index=False)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Test Predictions\", dataframe=test_df)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Return the best parameters and RMSE\n",
    "val_rmse, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e68232ce-8098-4809-8dd9-34ca689da12b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 135 candidates, totalling 675 fits\n",
      "Validation RMSE: 38440.0099609417\n",
      "Validation R^2: 0.8073570372518514\n",
      "Best Parameters: {'model__max_depth': 7, 'model__max_features': None, 'model__min_samples_leaf': 2, 'model__min_samples_split': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38440.0099609417,\n",
       " 0.8073570372518514,\n",
       " {'model__max_depth': 7,\n",
       "  'model__max_features': None,\n",
       "  'model__min_samples_leaf': 2,\n",
       "  'model__min_samples_split': 5})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "# Create new features\n",
    "train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "\n",
    "train_df['OverallQual_GrLivArea'] = train_df['OverallQual'] * train_df['GrLivArea']\n",
    "test_df['OverallQual_GrLivArea'] = test_df['OverallQual'] * test_df['GrLivArea']\n",
    "\n",
    "train_df['GrLivArea'] = np.log1p(train_df['GrLivArea'])\n",
    "test_df['GrLivArea'] = np.log1p(test_df['GrLivArea'])\n",
    "\n",
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "\n",
    "train_df['TotalPorchSF'] = (train_df['OpenPorchSF'] + train_df['3SsnPorch'] +\n",
    "                            train_df['EnclosedPorch'] + train_df['ScreenPorch'] + train_df['WoodDeckSF'])\n",
    "test_df['TotalPorchSF'] = (test_df['OpenPorchSF'] + test_df['3SsnPorch'] +\n",
    "                           test_df['EnclosedPorch'] + test_df['ScreenPorch'] + test_df['WoodDeckSF'])\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id', 'SalePrice'], axis=1, errors='ignore')\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create and evaluate the pipeline with grid search\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['sqrt', 'log2', None]  # 'auto' removed to avoid warning\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Predict on test set\n",
    "test_df['SalePrice'] = best_model.predict(X_test)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('test_predictions102.csv', index=False)\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Test Predictions\", dataframe=test_df)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "print(f\"Validation R^2: {val_r2}\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Return the best parameters, RMSE, and R^2 score\n",
    "val_rmse, val_r2, grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "18c2567a-f8bb-42a1-b16e-492a73b0543a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Validation RMSE for Random Forest: 31292.242082808014\n",
      "Validation R² for Random Forest: 0.8723386218370293\n",
      "Best Parameters for Random Forest: {'model__max_depth': 20, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31292.242082808014,\n",
       " 0.8723386218370293,\n",
       " {'model__max_depth': 20,\n",
       "  'model__max_features': 'sqrt',\n",
       "  'model__min_samples_leaf': 1,\n",
       "  'model__min_samples_split': 2,\n",
       "  'model__n_estimators': 300})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "# Create new features\n",
    "train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "\n",
    "train_df['OverallQual_GrLivArea'] = train_df['OverallQual'] * train_df['GrLivArea']\n",
    "test_df['OverallQual_GrLivArea'] = test_df['OverallQual'] * test_df['GrLivArea']\n",
    "\n",
    "train_df['GrLivArea'] = np.log1p(train_df['GrLivArea'])\n",
    "test_df['GrLivArea'] = np.log1p(test_df['GrLivArea'])\n",
    "\n",
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "\n",
    "train_df['TotalPorchSF'] = (train_df['OpenPorchSF'] + train_df['3SsnPorch'] +\n",
    "                            train_df['EnclosedPorch'] + train_df['ScreenPorch'] + train_df['WoodDeckSF'])\n",
    "test_df['TotalPorchSF'] = (test_df['OpenPorchSF'] + test_df['3SsnPorch'] +\n",
    "                           test_df['EnclosedPorch'] + test_df['ScreenPorch'] + test_df['WoodDeckSF'])\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "#X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "#y = train_df['SalePrice']\n",
    "#X_test = test_df.drop(['Id'], axis=1)\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id', 'SalePrice'], axis=1, errors='ignore')\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create and evaluate the pipeline with grid search\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred_rf = best_rf_model.predict(X_val)\n",
    "val_mse_rf = mean_squared_error(y_val, y_val_pred_rf)\n",
    "val_rmse_rf = np.sqrt(val_mse_rf)\n",
    "val_r2_rf = r2_score(y_val, y_val_pred_rf)\n",
    "\n",
    "# Predict on test set\n",
    "test_df['SalePrice'] = best_rf_model.predict(X_test)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('test_predictions_rf_corrected.csv', index=False)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation RMSE for Random Forest: {val_rmse_rf}\")\n",
    "print(f\"Validation R² for Random Forest: {val_r2_rf}\")\n",
    "print(f\"Best Parameters for Random Forest: {grid_search_rf.best_params_}\")\n",
    "\n",
    "# Return the best parameters and evaluation metrics\n",
    "val_rmse_rf, val_r2_rf, grid_search_rf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4804e2b9-ad62-494e-b341-2cca53406c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
      "Validation RMSE for Gradient Boosting: 25302.03237578527\n",
      "Validation R² for Gradient Boosting: 0.916536425563567\n",
      "Best Parameters for Gradient Boosting: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 4, 'model__min_samples_split': 2, 'model__n_estimators': 300}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25302.03237578527,\n",
       " 0.916536425563567,\n",
       " {'model__learning_rate': 0.1,\n",
       "  'model__max_depth': 3,\n",
       "  'model__max_features': 'sqrt',\n",
       "  'model__min_samples_leaf': 4,\n",
       "  'model__min_samples_split': 2,\n",
       "  'model__n_estimators': 300})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "# Create new features\n",
    "train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "\n",
    "train_df['OverallQual_GrLivArea'] = train_df['OverallQual'] * train_df['GrLivArea']\n",
    "test_df['OverallQual_GrLivArea'] = test_df['OverallQual'] * test_df['GrLivArea']\n",
    "\n",
    "train_df['GrLivArea'] = np.log1p(train_df['GrLivArea'])\n",
    "test_df['GrLivArea'] = np.log1p(test_df['GrLivArea'])\n",
    "\n",
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "\n",
    "train_df['TotalPorchSF'] = (train_df['OpenPorchSF'] + train_df['3SsnPorch'] +\n",
    "                            train_df['EnclosedPorch'] + train_df['ScreenPorch'] + train_df['WoodDeckSF'])\n",
    "test_df['TotalPorchSF'] = (test_df['OpenPorchSF'] + test_df['3SsnPorch'] +\n",
    "                           test_df['EnclosedPorch'] + test_df['ScreenPorch'] + test_df['WoodDeckSF'])\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "#X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "#y = train_df['SalePrice']\n",
    "#X_test = test_df.drop(['Id'], axis=1)\n",
    "\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id', 'SalePrice'], axis=1, errors='ignore')\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Create and evaluate the pipeline with grid search\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', gb_model)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred_gb = best_gb_model.predict(X_val)\n",
    "val_mse_gb = mean_squared_error(y_val, y_val_pred_gb)\n",
    "val_rmse_gb = np.sqrt(val_mse_gb)\n",
    "val_r2_gb = r2_score(y_val, y_val_pred_gb)\n",
    "\n",
    "# Predict on test set\n",
    "test_df['SalePrice'] = best_gb_model.predict(X_test)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('test_predictions_gb.csv', index=False)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation RMSE for Gradient Boosting: {val_rmse_gb}\")\n",
    "print(f\"Validation R² for Gradient Boosting: {val_r2_gb}\")\n",
    "print(f\"Best Parameters for Gradient Boosting: {grid_search_gb.best_params_}\")\n",
    "\n",
    "# Return the best parameters and evaluation metrics\n",
    "val_rmse_gb, val_r2_gb, grid_search_gb.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "668b654e-306e-423e-ad50-9de286ab3feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Validation RMSE for Voting Regressor: 27058.75672296335\n",
      "Validation R² for Voting Regressor: 0.9045443054682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27058.75672296335,\n",
       " 0.9045443054682,\n",
       " {'model__max_depth': 20,\n",
       "  'model__min_samples_leaf': 2,\n",
       "  'model__min_samples_split': 2,\n",
       "  'model__n_estimators': 200},\n",
       " {'model__learning_rate': 0.1,\n",
       "  'model__max_depth': 3,\n",
       "  'model__min_samples_leaf': 2,\n",
       "  'model__min_samples_split': 5,\n",
       "  'model__n_estimators': 200})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "\n",
    "# Create new features\n",
    "train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['TotalSF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "\n",
    "train_df['OverallQual_GrLivArea'] = train_df['OverallQual'] * train_df['GrLivArea']\n",
    "test_df['OverallQual_GrLivArea'] = test_df['OverallQual'] * test_df['GrLivArea']\n",
    "\n",
    "train_df['GrLivArea'] = np.log1p(train_df['GrLivArea'])\n",
    "test_df['GrLivArea'] = np.log1p(test_df['GrLivArea'])\n",
    "\n",
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "\n",
    "train_df['TotalPorchSF'] = (train_df['OpenPorchSF'] + train_df['3SsnPorch'] +\n",
    "                            train_df['EnclosedPorch'] + train_df['ScreenPorch'] + train_df['WoodDeckSF'])\n",
    "test_df['TotalPorchSF'] = (test_df['OpenPorchSF'] + test_df['3SsnPorch'] +\n",
    "                           test_df['EnclosedPorch'] + test_df['ScreenPorch'] + test_df['WoodDeckSF'])\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "train_df = pd.get_dummies(train_df, columns=categorical_cols)\n",
    "test_df = pd.get_dummies(test_df, columns=categorical_cols)\n",
    "train_df, test_df = train_df.align(test_df, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Separate features and target variable from training data\n",
    "#X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "#y = train_df['SalePrice']\n",
    "#X_test = test_df.drop(['Id'], axis=1)\n",
    "\n",
    "X = train_df.drop(['SalePrice', 'Id'], axis=1)\n",
    "y = train_df['SalePrice']\n",
    "X_test = test_df.drop(['Id', 'SalePrice'], axis=1, errors='ignore')\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Create pipelines\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "pipe_gb = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', gb_model)\n",
    "])\n",
    "\n",
    "# Define parameter grids\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__max_depth': [3, 4],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "grid_search_gb = GridSearchCV(pipe_gb, param_grid_gb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best models\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "\n",
    "# Create a Voting Regressor with both models\n",
    "voting_reg = VotingRegressor([('rf', best_rf_model), ('gb', best_gb_model)])\n",
    "voting_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred_voting = voting_reg.predict(X_val)\n",
    "val_mse_voting = mean_squared_error(y_val, y_val_pred_voting)\n",
    "val_rmse_voting = np.sqrt(val_mse_voting)\n",
    "val_r2_voting = r2_score(y_val, y_val_pred_voting)\n",
    "\n",
    "# Predict on test set\n",
    "test_df['SalePrice'] = voting_reg.predict(X_test)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "test_df[['Id', 'SalePrice']].to_csv('test_predictions_voting.csv', index=False)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Validation RMSE for Voting Regressor: {val_rmse_voting}\")\n",
    "print(f\"Validation R² for Voting Regressor: {val_r2_voting}\")\n",
    "\n",
    "# Return the best parameters and evaluation metrics\n",
    "val_rmse_voting, val_r2_voting, grid_search_rf.best_params_, grid_search_gb.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b092f-b422-46aa-98a0-60bc10efcc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
